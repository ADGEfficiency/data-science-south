---
id: talk
aliases: []
tags: []
competencies:
  - Analytics
title: Anomaly Detection
description: Find interesting data with rules, distance and machine learning based anomaly detection.
date: 2025-08-16
---

{{< quote text="An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism." author="D.M. Hawkins (1980) Identification of Outliers" >}}

Anomaly detection is a powerful set of techniques that belong in every data scientists toolset.

## What is Anomaly Detection?

Anomaly detection is the identification of unusual data points that deviate significantly from expected patterns or normal behavior.

These unusual observations can indicate:

- **Data quality issues**: Missing values, measurement errors, or corrupted records
- **Interesting phenomena**: Novel discoveries, rare events, or emerging trends  
- **System problems**: Equipment failures, security breaches, or process breakdowns
- **Different data processes**: Points generated by mechanisms different from the majority

**Understanding your domain is crucial for effective anomaly detection.** The definition of "normal" depends entirely on context - what's anomalous in financial transactions differs from what's anomalous in network traffic or medical data. 

### What is an Anomaly Detector?

**An anomaly detector is an algorithm that automatically identifies records as anomalies** based on statistical, distance-based, or machine learning techniques.

The choice of detector depends on your data characteristics, domain knowledge, computational constraints, and interpretability requirements.

### Resources

Resources to learn anomaly detection:

- **PyOD**: Python library with 40+ anomaly detection algorithms [pyod.readthedocs.io](https://pyod.readthedocs.io/)
- **Numenta Anomaly Benchmark**: Real-world time series anomaly detection benchmark [github.com/numenta/NAB](https://github.com/numenta/NAB)

## Why Learn Anomaly Detection?

Anomaly detection enables you to:

- **Find Interesting Data**: Identify rare events, novel patterns, and unusual behaviors
- **Clean Data**: Find and fix data quality issues
- **Label Data**: Label anomalies for supervised learning

Anomaly detection methods apply across manyp domains - the same statistical methods that detect credit card fraud can find manufacturing defects or identify unusual patient symptoms.

Practical applications of anomaly detection include:

- **Prevent financial fraud**: Detect unusual transaction patterns in banking, credit cards, and online payments before losses occur
- **Monitor system health**: Identify server failures, network intrusions, and performance degradation in real-time
- **Ensure quality control**: Find defective products, manufacturing errors, and quality issues in production lines
- **Enable predictive maintenance**: Detect equipment failures before they happen, reducing downtime and maintenance costs
- **Enhance cybersecurity**: Identify malicious activities, data breaches, and security threats in network traffic
- **Improve healthcare**: Detect medical anomalies in patient data, unusual symptoms, and diagnostic outliers

## Types of Outliers

### Strong vs. Weak Outliers

If the outliers are only outliers because of noise, they are weak outliers. Strong outliers are outliers because they are generated by different data generating processes.

### Internal vs. External Outliers

Internal outliers are less common than extreme values. They appear only in multimodal distributions or distributions with gaps.

Internal outliers can be detected by the following algorithms, which are more flexible to unusual distributions:

- **Histograms**: Count-based methods
- **Kernel Density Estimation (KDE)**: Probability distribution density estimation
- **Nearest Neighbour methods**: Distance-based methods

### Local vs. Global Outliers

**Global outliers are unusual when compared to the entire dataset**. These points are far from all other data points and would be considered anomalies regardless of local context.

**Local outliers are unusual only within their local neighborhood**, but may appear normal when viewed globally. A point might be close to one cluster but far from the specific cluster it should belong to.

Local outliers are also known as in-distribution anomalies, while global outliers are out-of-distribution anomalies.

## Masking & Swamping

### Masking

**Masking occurs when the presence of outliers causes other outliers to not be detected**. This happens because extreme outliers can shift statistical measures (like mean and standard deviation) so much that other outliers appear normal by comparison.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Generate normal data with two types of outliers
np.random.seed(42)
normal_data = np.random.normal(0, 1, 100)
moderate_outliers = [3.5, 3.7]  # These should be detected
extreme_outliers = [15, 16]     # These mask the moderate ones

data = np.concatenate([normal_data, moderate_outliers, extreme_outliers])

# Z-score detection (affected by masking)
z_scores = np.abs(stats.zscore(data))
outliers_zscore = data[z_scores > 2]

# Robust detection using MAD
mad = np.median(np.abs(data - np.median(data)))
modified_z_scores = 0.6745 * (data - np.median(data)) / mad
outliers_mad = data[np.abs(modified_z_scores) > 2]

print(f"Z-score outliers: {outliers_zscore}")
print(f"MAD outliers: {outliers_mad}")
```

```output
Z-score outliers: [15. 16.]
MAD outliers: [-1.91328024 -1.72491783  1.85227818 -1.95967012 -1.76304016 -2.6197451
 -1.98756891  3.5         3.7        15.         16.        ]
```

Moderate outliers [3.5, 3.7] missed by Z-score due to masking from extreme outliers [15, 16]

![Masking Example](/images/masking_example.png)

### Swamping

**Swamping occurs when the presence of outliers causes normal points to be incorrectly identified as outliers**. This typically happens when outliers inflate the variance, making the detection threshold too sensitive.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Generate clustered data with one extreme outlier
np.random.seed(42)
cluster1 = np.random.normal(0, 0.5, 50)
cluster2 = np.random.normal(5, 0.5, 50)
extreme_outlier = [20]  # This will cause swamping

data = np.concatenate([cluster1, cluster2, extreme_outlier])

# Z-score detection (affected by swamping)
z_scores = np.abs(stats.zscore(data))
outliers_zscore = data[z_scores > 2]

# Robust detection using median and MAD
median_val = np.median(data)
mad = np.median(np.abs(data - median_val))
modified_z_scores = 0.6745 * (data - median_val) / mad
outliers_mad = data[np.abs(modified_z_scores) > 2]

print(f"Z-score outliers (swamping): {len(outliers_zscore)} points")
print(f"MAD outliers (robust): {len(outliers_mad)} points")
```

```output
Z-score outliers (swamping): 1 points
MAD outliers (robust): 1 points
Z-score detected: [20.]
MAD detected: [20.]
```

The extreme outlier 20 inflated variance, causing normal points to be flagged as outliers

![Swamping Example](/images/swamping_example.png)

## Labels & Scores

**An anomaly label is a binary classification of whether a record is an anomaly or not**.  Generating a label is often done with a score and a threshold.

**An anomaly score is a continuous value that indicates how abnormal a record is**.  Example of scores include a z score, a distance from a cluster, or a density estimate.

Scores can be combined and ranked, although care is needed when combining scores on different scales.  Scores can be combined with the sum, maximum or sum of squares.

## Univariate vs. Multivariate Anomaly Detection

**Univariate outlier detection uses a single feature to find unusual values**. The scores created with univariate anomaly detection can be combined for all features in a row, to generate an estimate for the entire row. 

Univariate outiler detection has one source of unusualness - unusual because of the single feature. A univariate outlier detection model cannot understand the relationships between features. 

**Multivariate outlier detection uses many features**, and can find unusual combinations of unusual values alongside just finding unusual values. 

Multivariate outlier detection has two sources of unusualness - unusual because of the single feature and/or unusual when combined with other features.

## Curse of Dimensionality

The curse of dimensionality refers to the problems that arise when working with high-dimensional data.

Very few outliers require many features to detect them.  Some outliers can only be detected in high dimensions.

For outlier detection, high dimensions cause the following issues:

- **Interpretability**: Outliers that relate to many features are less interpretable
- **Sparsity**: High dimensional data can become spares, where many combinations cannot be meaningfully populated.
- **More ways to be unusual**: Increasing dimensionality creates more opportunities for data points to stand out as anomalies

## Categorical Features

One common way to detect anomalies with categorical variables is with counts.  Anomalies can then be detected by thresholds on the counts.

Can set a threshold on a cumulative count. Here you would set a record to an anomaly based on the cumulative count of all values of that column, stopping where the total is equal to a threshold of 1% (for example).

You can also do outlier detection on the counts - for example using MAD or z score on the counts.

### Marginal Probabilities

Marginal probabilities detect anomalies in categorical features by identifying values with unusually low probability of occurrence. This method calculates the probability of each category value and flags records containing rare combinations.

The marginal probabilities method is particularly effective for:

- **High-cardinality categorical features**: Where many categories have low frequency
- **Multi-categorical records**: Detecting unusual combinations across multiple categorical features  
- **Interpretability**: Easy to explain why a record is anomalous based on rare category values

Basic marginal probability calculation:

```python
import pandas as pd
import numpy as np

# Sample categorical data
data = pd.DataFrame({
    'department': ['Sales', 'Sales', 'Engineering', 'Engineering', 'Marketing', 'Legal'],
    'location': ['NYC', 'NYC', 'SF', 'SF', 'NYC', 'Remote'],
    'level': ['Junior', 'Senior', 'Senior', 'Senior', 'Senior', 'Partner']
})

# Calculate marginal probabilities for each feature
dept_probs = data['department'].value_counts(normalize=True)
location_probs = data['location'].value_counts(normalize=True)
level_probs = data['level'].value_counts(normalize=True)

print("Department probabilities:")
print(dept_probs)
print("\nLocation probabilities:")
print(location_probs)
print("\nLevel probabilities:")
print(level_probs)
```

```output
Department probabilities:
department
Sales          0.333333
Engineering    0.333333
Marketing      0.166667
Legal          0.166667

Location probabilities:
location
NYC       0.5
SF        0.333333
Remote    0.166667

Level probabilities:
level
Senior     0.666667
Junior     0.166667
Partner    0.166667
```

Detecting anomalies using probability thresholds:

```python
# Set threshold for rare categories (probability < 0.2)
threshold = 0.2

# Find records with rare categorical values
anomalies = []
for idx, row in data.iterrows():
    dept_prob = dept_probs[row['department']]
    loc_prob = location_probs[row['location']]
    level_prob = level_probs[row['level']]
    
    # Flag if any category has probability below threshold
    if dept_prob < threshold or loc_prob < threshold or level_prob < threshold:
        anomalies.append({
            'index': idx,
            'department': row['department'],
            'location': row['location'], 
            'level': row['level'],
            'dept_prob': dept_prob,
            'loc_prob': loc_prob,
            'level_prob': level_prob
        })

print(f"Found {len(anomalies)} anomalous records:")
for anomaly in anomalies:
    print(f"Row {anomaly['index']}: {anomaly['department']}, {anomaly['location']}, {anomaly['level']}")
    print(f"  Probabilities: dept={anomaly['dept_prob']:.3f}, loc={anomaly['loc_prob']:.3f}, level={anomaly['level_prob']:.3f}")
```

```output
Found 3 anomalous records:
Row 4: Marketing, NYC, Senior
  Probabilities: dept=0.167, loc=0.500, level=0.667
Row 5: Legal, Remote, Partner
  Probabilities: dept=0.167, loc=0.167, level=0.167
Row 0: Sales, NYC, Junior
  Probabilities: dept=0.333, loc=0.500, level=0.167
```

Combined probability scoring:

```python
# Calculate combined probability score for each record
data['combined_prob'] = data.apply(lambda row: 
    dept_probs[row['department']] * 
    location_probs[row['location']] * 
    level_probs[row['level']], axis=1)

# Sort by combined probability (lowest = most anomalous)
data_sorted = data.sort_values('combined_prob')
print("Records sorted by combined probability (most anomalous first):")
print(data_sorted[['department', 'location', 'level', 'combined_prob']])
```

```output
Records sorted by combined probability (most anomalous first):
  department location    level  combined_prob
5      Legal   Remote  Partner       0.004630
0      Sales      NYC   Junior       0.027778
4  Marketing      NYC   Senior       0.055556
1      Sales      NYC   Senior       0.111111
2 Engineering       SF   Senior       0.148148
3 Engineering       SF   Senior       0.148148
```

## Visualizing Anomalies

Visualization is crucial for understanding your data and validating anomaly detection results. Different chart types work best for different data combinations.

### Single Variable Visualizations

**Histogram**: Shows the distribution and identifies gaps or unusual peaks

```python
data = [1, 2, 3, 4, 5, 100]  # 100 is an outlier
plt.hist(data, bins=10, alpha=0.7, edgecolor='black')
```

![Histogram](/images/anomaly/histogram.png)

**Kernel Density Estimation (KDE)**: Smooth density curves that highlight low-density regions

```python
sns.histplot(data, kde=True, alpha=0.7)
```

![KDE Plot](/images/anomaly/kde.png)

**Boxplot**: Identifies outliers using quartiles and IQR

```python
plt.boxplot(data)  # Points beyond whiskers are outliers
```

![Boxplot](/images/anomaly/boxplot.png)

### Two Variable Visualizations

**Scatterplot**: Reveals outlying points and clusters in two numeric variables

```python
x = [1, 2, 3, 4, 10]
y = [1, 2, 3, 4, 10]  # (10, 10) is an outlier
plt.scatter(x, y)
```

![Scatterplot](/images/anomaly/scatterplot.png)

**Heatmap**: Shows frequency patterns between two categorical variables

```python
df = pd.DataFrame({'cat1': ['A', 'A', 'B'], 'cat2': ['X', 'Y', 'Z']})
heatmap_data = pd.crosstab(df['cat1'], df['cat2'])
sns.heatmap(heatmap_data, annot=True)
```

![Heatmap](/images/anomaly/heatmap.png)

**Boxplot by category**: Identifies outliers within categorical groups

```python
data_by_group = [[1, 2, 3, 4], [10, 11, 12, 100]]  # 100 is outlier in group 2
plt.boxplot(data_by_group, tick_labels=['Group A', 'Group B'])
```

![Boxplot by Category](/images/anomaly/boxplot_by_category.png)

### Advanced Visualizations

**Z-score visualization**: Color-codes points by their statistical outlier scores

```python
z_scores = np.abs(stats.zscore(data))
plt.scatter(range(len(data)), data, c=z_scores, cmap='Reds')
plt.colorbar(label='|Z-score|')
```

![Z-score Visualization](/images/anomaly/zscore.png)

**Density contour plot**: Shows bivariate density with outliers highlighted

```python
plt.scatter(normal_points[:, 0], normal_points[:, 1], label='Normal')
plt.scatter(outlier_points[:, 0], outlier_points[:, 1], color='red', label='Outliers')
plt.legend()
```

![Density Contour Plot](/images/anomaly/density_contour.png)

## Anomaly Detection Methods

Anomaly detection methods can be categorized into three main approaches, each with different strengths and use cases:

- **Rules-based**: Simple, interpretable thresholds and conditions
- **Statistical**: Mathematical approaches using distributions and probabilities  
- **Machine Learning**: Advanced algorithms that learn patterns from data

The choice of method depends on your data characteristics, domain knowledge, interpretability requirements, and computational constraints.

## Rules-Based Detection

Rules-based anomaly detection uses predefined thresholds and logical conditions to identify outliers. These methods are highly interpretable and easy to implement.

**Advantages:**
- **Interpretable**: Easy to understand and explain to stakeholders
- **Fast**: Simple comparisons with minimal computation
- **Customizable**: Can incorporate domain expertise and business logic
- **Deterministic**: Same input always produces same result

**Disadvantages:**
- **Manual effort**: Requires domain knowledge to set appropriate thresholds
- **Rigid**: May miss novel anomaly patterns
- **Complexity explosion**: Multivariate rules become difficult to manage with many features
- **Threshold sensitivity**: Performance depends heavily on chosen thresholds

### Univariate Rules

Simple threshold-based rules applied to single variables:

**Range rules**: Flag values outside expected ranges

```python
ages = [25, 30, -5, 150, 40]
outliers = [age for age in ages if age < 0 or age > 120]
print(f"Age outliers: {outliers}")
```
```output
Age outliers: [-5, 150]
```

**Percentage rules**: Flag extreme percentiles

```python
import numpy as np
data = np.array([10, 12, 15, 18, 20, 22, 25, 28, 30, 100])
outliers = data[data > np.percentile(data, 95)]
print(f"Top 5% outliers: {outliers}")
```
```output
Top 5% outliers: [100]
```

**Business logic rules**: Domain-specific constraints

```python
employees = [{'salary': 80000, 'revenue': 70000}, {'salary': 50000, 'revenue': 200000}]
outliers = [emp for emp in employees if emp['salary'] > emp['revenue']]
print(f"Salary > revenue: {len(outliers)} employees")
```
```output
Salary > revenue: 1 employees
```

### Multivariate Rules

Multivariate rules are combinations of univariate rules.  These can do things like check for consistency (that a start date is before an end date) or on ratios of features.

Combining multivariate rules is not straightforward, as the number of combinations grows exponentially with the number of features.

## Statistical Methods

Statistical anomaly detection uses mathematical properties of data distributions to identify outliers. These methods assume data follows certain statistical patterns and flag points that deviate significantly from these patterns.

**Advantages of Statistical Methods:**
- **Well-understood**: Based on established statistical theory
- **Computationally efficient**: Simple calculations, fast execution
- **Threshold interpretability**: Scores have statistical meaning
- **Robust options available**: MAD and IQR handle outliers well

**Disadvantages:**
- **Distribution assumptions**: Many methods assume normal distributions
- **Univariate focus**: Don't capture relationships between variables
- **Parameter sensitivity**: Threshold choice affects performance significantly

### Z-Score (Standard Score)

Measures how many standard deviations a point is from the mean. Points with $|z\text{-score}| > 2$ or $3$ are typically considered outliers.

$$z = \frac{x - \mu}{\sigma}$$

Where:
- $x$ is the data point
- $\mu$ is the mean
- $\sigma$ is the standard deviation

**Advantages:** Simple, well-understood, works well for normal distributions  
**Disadvantages:** Sensitive to extreme outliers (masking/swamping), assumes normal distribution

```python
import numpy as np
from scipy import stats

data = [1, 2, 3, 4, 5, 100]
z_scores = np.abs(stats.zscore(data))
outliers = [x for x, z in zip(data, z_scores) if z > 2]
print(f"Outliers: {outliers}")  # [100]
```

### Interquartile Range (IQR)

Uses quartiles to define outlier boundaries. Points outside $Q_1 - 1.5 \times IQR$ or $Q_3 + 1.5 \times IQR$ are flagged.

$$IQR = Q_3 - Q_1$$

Outliers if: $x < Q_1 - 1.5 \times IQR$ or $x > Q_3 + 1.5 \times IQR$

**Advantages:** Robust to outliers, doesn't assume normal distribution  
**Disadvantages:** Fixed multiplier (1.5) may not suit all datasets

```python
import numpy as np

data = [1, 2, 3, 4, 5, 100]
Q1, Q3 = np.percentile(data, [25, 75])
IQR = Q3 - Q1
outliers = [x for x in data if x < Q1 - 1.5*IQR or x > Q3 + 1.5*IQR]
print(f"Outliers: {outliers}")  # [100]
```

### Median Absolute Deviation (MAD)

More robust alternative to standard deviation, using median instead of mean.

$$MAD = \text{median}(|x_i - \text{median}(x)|)$$

**Advantages:** Highly robust to outliers, works with skewed distributions  
**Disadvantages:** Less intuitive than z-score, requires threshold tuning

```python
import numpy as np

data = [1, 2, 3, 4, 5, 100]
median = np.median(data)
mad = np.median(np.abs(data - median))
outliers = [x for x in data if abs(x - median) > 3 * mad]
print(f"Outliers: {outliers}")  # [100]
```

### Modified Z-Score

Uses MAD instead of standard deviation for more robust outlier detection.

$$\text{Modified Z} = \frac{0.6745 \times (x - \text{median}(x))}{MAD}$$

The constant 0.6745 makes MAD equivalent to standard deviation for normal distributions.

**Advantages:** Combines interpretability of z-score with robustness of MAD  
**Disadvantages:** Constant (0.6745) assumes normal distribution

### Combining Statistical Methods

Statistical scores can be combined across multiple features to create multivariate outlier detection:

- **Sum of scores**: Add z-scores across features
- **Maximum score**: Take the highest absolute score across features  
- **Sum of squares**: Combine squared z-scores (similar to Mahalanobis distance)

## Machine Learning Methods

Machine learning approaches to anomaly detection learn patterns from data and can handle complex, multivariate relationships. These algorithms are typically more sophisticated but less interpretable than rules or statistical methods.

We will look at a few different groups of machine learning methods:

- **Distance-based methods**: Identify outliers based on distance to other points
- **Density-based methods**: Identify outliers in low-density regions
- **Clustering-based methods**: Use clustering algorithms to find outliers
- **Tree-based methods**: Use decision trees to isolate anomalies
- **Probabilistic methods**: Model data distributions and flag low-probability points
- **Histogram-based methods**: Use histograms to identify outliers

**Advantages of Machine Learning Methods:**
- **Multivariate**: Capture complex relationships between features
- **Adaptive**: Learn patterns from data without manual threshold setting
- **Flexible**: Handle non-linear relationships and arbitrary distributions
- **Scalable**: Many algorithms work well with large datasets

**Disadvantages:**
- **Less interpretable**: Harder to explain why a point is flagged
- **Parameter tuning**: Requires hyperparameter optimization
- **Computational cost**: More expensive than statistical methods
- **Overfitting risk**: May learn noise patterns in training data
- **Data requirements**: Need sufficient training data for reliable patterns

### Distance-Based Methods

Identify outliers based on their distance to other points or clusters.

**k-Nearest Neighbors (k-NN):**
- Flag points with large distances to their k nearest neighbors
- Can use mean, median, or maximum distance to k neighbors
- Works well when normal points form dense clusters

**Advantages:**

- **Simple concept**: Easy to understand and implement
- **Arbitrary cluster shapes**: Handles non-spherical clusters well
- **No distribution assumptions**: Works with any data distribution

**Disadvantages:**

- **Computationally expensive**: O(nÂ²) distance calculations for each point
- **Parameter sensitivity**: Performance highly dependent on k value
- **Varying density struggles**: Poor performance when clusters have different densities

### Density-Based Methods

Identify outliers as points in low-density regions.

**Local Outlier Factor (LOF):**
- Compares local density of a point to densities of its neighbors
- Points in sparser regions relative to neighbors are flagged
- Handles clusters of different densities well

**Kernel Density Estimation (KDE):**
- Estimates probability density function and flags low-probability points
- Can capture complex data distributions

**Advantages:**

- **Varying cluster densities**: Handles clusters with different densities well
- **Local anomaly detection**: Finds outliers relative to local neighborhoods
- **Complex distributions**: Captures non-parametric data patterns

**Disadvantages:**

- **Parameter tuning required**: Bandwidth selection critically affects performance
- **Computationally intensive**: Expensive for large datasets
- **Curse of dimensionality**: Performance degrades in high dimensions

### Clustering-Based Methods

Use clustering algorithms to identify outliers as points that don't belong to any cluster or are far from cluster centers.

**k-Means Outliers:**
- Points far from nearest cluster centroid
- Points in small or sparse clusters

**DBSCAN Outliers:**
- Points classified as "noise" by the algorithm
- Points not belonging to any dense cluster

**Advantages:**

- **Leverages existing algorithms**: Uses well-established clustering methods
- **Intuitive concept**: Easy to understand outliers as non-clustered points
- **Natural grouping**: Identifies outliers based on natural data clusters

**Disadvantages:**

- **Clustering quality dependence**: Performance tied to underlying clustering success
- **Parameter sensitivity**: Requires careful tuning of clustering parameters
- **Cluster assumption**: Assumes data naturally forms clusters

### Tree-Based Methods

**Isolation Forest:**
- Isolates anomalies by randomly partitioning data
- Anomalies require fewer partitions to isolate (shorter paths in trees)
- Ensemble of isolation trees votes on anomaly score

**Advantages:**

- **Fast execution**: Linear time complexity, scales well
- **Large dataset handling**: Efficient memory usage and processing
- **No distribution assumptions**: Works with any data distribution
- **Feature independence**: Handles mixed data types well

**Disadvantages:**

- **Less interpretable**: Difficult to explain why specific points are outliers
- **Sparse region struggles**: May flag normal points in low-density areas
- **Random variation**: Results can vary between runs due to randomness


### Probabilistic Methods

**Gaussian Mixture Models (GMM):**
- Model data as mixture of Gaussian distributions
- Flag points with low likelihood under the learned model
- Can capture multiple modes in data

**One-Class SVM:**
- Learn boundary around normal data in high-dimensional space
- Flag points outside the learned boundary

**Advantages:**

- **Principled framework**: Based on solid probabilistic theory
- **Complex distributions**: Handles multimodal and non-linear patterns
- **Uncertainty quantification**: Provides confidence scores for predictions

**Disadvantages:**

- **Strong assumptions**: Assumes Gaussian mixture distributions
- **Computationally intensive**: Expensive training and inference
- **Parameter selection**: Requires choosing number of components


### Histogram-Based Methods

**Histogram-Based Outlier Score (HBOS):**
- Build histograms for each feature independently
- Combine scores across features (assumes feature independence)
- Fast and interpretable for high-dimensional data

**Advantages:**

- **Fast execution**: Linear time complexity for training and prediction
- **Scales well**: Handles high-dimensional data efficiently
- **Interpretable contributions**: Easy to understand which features drive anomaly scores
- **Memory efficient**: Low memory requirements compared to distance-based methods

**Disadvantages:**

- **Feature independence assumption**: Ignores correlations between features
- **Multivariate pattern blindness**: May miss complex feature interactions
- **Bin size sensitivity**: Performance depends on histogram bin selection

## Practical Tips

### Data Volume Requirements

Anomaly detection methods generally work best with substantial datasets:

- **Statistical methods**: Need sufficient samples to estimate parameters reliably
- **Machine learning methods**: Require enough data to learn normal patterns
- **Minimum recommendations**: At least 1000+ records for robust detection

### Setting Realistic Expectations

Don't expect zero anomalies in real data:

- **Normal anomaly rates**: Expect 1-5% anomalies in typical datasets
- **Threshold accordingly**: Set contamination parameters based on expected rates
- **Domain knowledge**: Use business context to validate anomaly rates

```python
# Set realistic contamination rates
from pyod.models.iforest import IForest

# Expect 2% anomalies based on domain knowledge
clf = IForest(contamination=0.02)
outliers = clf.fit_predict(data)
```

### Data Separation Strategy

Separate different record types before applying anomaly detection:

- **Transaction types**: Analyze purchases and sales separately, not together
- **User segments**: Apply detection within user groups (new vs. returning customers)
- **Time periods**: Consider seasonal patterns by analyzing periods separately

```python
# Separate analysis by transaction type
purchase_data = transactions[transactions['type'] == 'purchase']
sale_data = transactions[transactions['type'] == 'sale']

# Run anomaly detection on each separately
purchase_outliers = detect_anomalies(purchase_data)
sale_outliers = detect_anomalies(sale_data)
```

### Feature Engineering Considerations

Prepare features appropriately for anomaly detection:

- **Scaling**: Normalize features to prevent dominance by large-scale variables
- **Encoding**: Handle categorical variables with appropriate encoding methods
- **Temporal features**: Extract time-based patterns (hour, day, seasonality)

### Ensemble Methods

Ensemble approaches combine multiple anomaly detectors for improved performance:

**Why use ensembles:**

- **Pattern diversity**: Different algorithms detect different types of anomalies
- **Variance reduction**: Averaging reduces impact of individual model errors
- **Reliability**: More stable and robust final anomaly scores

**Ensemble strategies:**

```python
from pyod.models.combination import aom, moa, average
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.hbos import HBOS

# Train multiple detectors
clf1 = IForest(contamination=0.1)
clf2 = LOF(contamination=0.1) 
clf3 = HBOS(contamination=0.1)

scores1 = clf1.fit(data).decision_scores_
scores2 = clf2.fit(data).decision_scores_
scores3 = clf3.fit(data).decision_scores_

# Combine scores using different methods
avg_scores = average([scores1, scores2, scores3])  # Simple average
aom_scores = aom([scores1, scores2, scores3])      # Average of maximums
moa_scores = moa([scores1, scores2, scores3])      # Maximum of averages
```

**Ensemble benefits:**

- **Complementary detection**: Statistical + ML methods catch different anomaly types
- **Reduced false positives**: Consensus voting filters spurious detections  
- **Improved recall**: Multiple detectors increase chance of catching true anomalies

## Explainability & Interpretability

Understanding why an anomaly detector flags certain records as outliers is crucial for building trust and actionable insights. The requirements for explanation vary significantly based on your audience and use case.

### Interpretability vs. Explainability

The distinction between interpretability and explainability exists on a spectrum rather than as rigid categories. Understanding this spectrum helps choose appropriate techniques for different audiences and use cases.

**Interpretability** generally refers to models that are more inherently understandable, where you can directly see how decisions are made with minimal additional tools.

**Explainability** typically involves external techniques to understand complex models, often applied after the model has made predictions.

```python
# High interpretability: Z-score threshold
z_score = (value - mean) / std
is_outlier = abs(z_score) > 2  # Direct, clear reasoning

# Lower interpretability: Isolation Forest
clf = IForest()
outlier_score = clf.decision_function([record])  # Needs explanation tools
```

**The spectrum in practice:**

- **High interpretability**: Z-score, IQR, simple business rules
- **Medium interpretability**: Decision trees, linear models with feature importance
- **Lower interpretability**: Isolation Forest, neural networks, ensemble methods
- **Requires explainability tools**: Complex ensembles, deep learning models

**Gray areas and considerations:**

- **Audience dependency**: What's interpretable to a data scientist may need explanation for business users
- **Context matters**: Tree models show rules but may be complex with many branches
- **Feature interactions**: Some "interpretable" methods miss important relationships
- **Timing**: Interpretability during model design vs. explainability applied post-hoc

### Global vs. Local Explanations

**Global explanations** describe how the model works overall across the entire dataset.

**Local explanations** describe why a specific record was flagged as an anomaly.

```python
# Global: Overall feature importance across all predictions
feature_importance = [0.4, 0.3, 0.2, 0.1]  # Shows which features matter most

# Local: Why this specific record is anomalous
record_explanation = {
    'salary': 'Extremely high (99th percentile)',
    'age': 'Normal range',
    'department': 'Rare combination with salary'
}
```

### Interpretable Methods

These methods are inherently understandable without additional explanation tools:

**Statistical methods:**

- **Z-score/MAD**: Clear threshold-based rules with statistical meaning
- **IQR**: Quartile-based boundaries with intuitive interpretation
- **Percentile-based**: Direct ranking interpretable to any audience

**Histogram-based methods:**

- **HBOS**: Shows which features have unusual value frequencies
- **Count-based detection**: Simple frequency thresholds easy to understand

```python
# Interpretable example: MAD-based detection
median_val = np.median(data)
mad = np.median(np.abs(data - median_val))
threshold = 3 * mad
outliers = data[np.abs(data - median_val) > threshold]
print(f"Outliers: values more than {threshold:.2f} units from median {median_val:.2f}")
```

**Rules-based methods:**

- **Business logic rules**: Domain-specific constraints anyone can understand
- **Range checks**: Simple min/max boundaries
- **Consistency checks**: Logical relationships between fields

### Explainability Techniques

For complex models that lack inherent interpretability, use these post-hoc explanation methods:

### Feature Importance

Shows which features contribute most to anomaly detection globally:

```python
from sklearn.inspection import permutation_importance
from pyod.models.iforest import IForest

# Train isolation forest
clf = IForest(contamination=0.1)
clf.fit(X_train)

# Calculate feature importance via permutation
perm_importance = permutation_importance(clf, X_test, scoring='roc_auc')
feature_names = ['age', 'salary', 'experience']

for i, importance in enumerate(perm_importance.importances_mean):
    print(f"{feature_names[i]}: {importance:.3f}")
```

```output
age: 0.023
salary: 0.156
experience: 0.087
```

**Limitations:** Feature importance doesn't capture feature interactions or individual record explanations.

### SHAP (SHapley Additive exPlanations)

Provides both global and local explanations with theoretical guarantees:

```python
import shap
from pyod.models.iforest import IForest

# Train model and get SHAP explainer
clf = IForest(contamination=0.1)
clf.fit(X_train)
explainer = shap.Explainer(clf.decision_function, X_train)

# Local explanation for specific record
shap_values = explainer(X_test[0:1])
print(f"Base value: {explainer.expected_value:.3f}")
print(f"SHAP values: {shap_values.values[0]}")
```

```output
Base value: 0.102
SHAP values: [-0.025, 0.089, 0.034]
```

### Proxy Models

Train simple, interpretable models to mimic complex model predictions:

```python
from sklearn.tree import DecisionTreeClassifier
from pyod.models.iforest import IForest

# Complex model predictions
iforest = IForest(contamination=0.1)
complex_predictions = iforest.fit_predict(X)

# Simple proxy model
proxy = DecisionTreeClassifier(max_depth=3)
proxy.fit(X, complex_predictions)

# Proxy provides interpretable rules
from sklearn.tree import export_text
rules = export_text(proxy, feature_names=['age', 'salary', 'experience'])
print(rules)
```

```output
|--- salary <= 75000.00
|   |--- age <= 25.00
|   |   |--- class: 0 (normal)
|   |--- age >  25.00
|   |   |--- experience <= 2.00
|   |   |   |--- class: 1 (anomaly)
```

### Visualization Techniques

**Partial Dependence Plots:** Show how anomaly scores change with individual features:

```python
from sklearn.inspection import PartialDependenceDisplay
import matplotlib.pyplot as plt

# Create partial dependence plot
PartialDependenceDisplay.from_estimator(clf, X, features=[0, 1], feature_names=['age', 'salary'])
plt.show()
```

**Individual Conditional Expectation (ICE) plots:** Show prediction changes for individual records across feature values.

### Counterfactual Explanations

Show minimum changes needed to change a prediction from anomaly to normal:

```python
# Pseudo-code for counterfactual generation
def generate_counterfactual(model, anomalous_record):
    """Find minimal changes to make record normal"""
    counterfactual = anomalous_record.copy()
    
    # Iteratively modify features with smallest changes
    for feature in features_by_importance:
        modified_record = counterfactual.copy()
        modified_record[feature] = find_normal_value(feature)
        
        if model.predict([modified_record])[0] == 0:  # Normal
            return modified_record, calculate_changes(anomalous_record, modified_record)
    
    return None

original = [25, 150000, 2]  # [age, salary, experience] - flagged as anomaly
counterfactual, changes = generate_counterfactual(model, original)
print(f"Change salary from {original[1]} to {counterfactual[1]} to be normal")
```

```output
Change salary from 150000 to 80000 to be normal
```

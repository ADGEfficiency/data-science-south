---
id: talk
aliases: []
tags: []
competencies:
  - Analytics
title: Anomaly Detection
description: Find interesting data with rules, distance and machine learning based anomaly detection.
date: 2025-08-16
---

> An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.
>
> Hawkins (1980)

## What is Anomaly Detection?

A few useful definitions:
- **Finding unusual items**: This is subjective
- **Finding interesting items**: Many outliers are not interesting, but many interesting things are outliers
- **Finding data generating processes**: Anomalies in data can be caused by different processes

If we expect multiple data processes, then the outliers we find are expected.

An anomaly detector is an algorithm that identifies records as anomalies. The choice of detector depends on your data, domain knowledge, and detection interpretability requirements.

### Strong vs. Weak Outliers

If the outliers are only outliers because of noise, they are weak outliers. Strong outliers are outliers because they are generated by different data generating processes.

### Internal vs. External Outliers

Internal outliers are less common than extreme values. They appear only in multimodal distributions or distributions with gaps.

Internal outliers can be detected by the following algorithms, which are more flexible to unusual distributions:

- Histograms
- Kernel Density Estimation (KDE)
- Nearest Neighbour methods

### Local vs. Global Outliers

**Global outliers** are unusual when compared to the entire dataset. These points are far from all other data points and would be considered anomalies regardless of local context.

**Local outliers** are unusual only within their local neighborhood, but may appear normal when viewed globally. A point might be close to one cluster but far from the specific cluster it should belong to.

Local outliers are also known as in-distribution anomalies, while global outliers are out-of-distribution anomalies.

### Masking

**Masking** occurs when the presence of outliers causes other outliers to not be detected. This happens because extreme outliers can shift statistical measures (like mean and standard deviation) so much that other outliers appear normal by comparison.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Generate normal data with two types of outliers
np.random.seed(42)
normal_data = np.random.normal(0, 1, 100)
moderate_outliers = [3.5, 3.7]  # These should be detected
extreme_outliers = [15, 16]     # These mask the moderate ones

data = np.concatenate([normal_data, moderate_outliers, extreme_outliers])

# Z-score detection (affected by masking)
z_scores = np.abs(stats.zscore(data))
outliers_zscore = data[z_scores > 2]

# Robust detection using MAD
mad = np.median(np.abs(data - np.median(data)))
modified_z_scores = 0.6745 * (data - np.median(data)) / mad
outliers_mad = data[np.abs(modified_z_scores) > 2]

print(f"Z-score outliers: {outliers_zscore}")
print(f"MAD outliers: {outliers_mad}")
```

```output
Z-score outliers: [15. 16.]
MAD outliers: [-1.91328024 -1.72491783  1.85227818 -1.95967012 -1.76304016 -2.6197451
 -1.98756891  3.5         3.7        15.         16.        ]
```

Moderate outliers [3.5, 3.7] missed by Z-score due to masking from extreme outliers [15, 16]

![Masking Example](/images/masking_example.png)

### Swamping

**Swamping** occurs when the presence of outliers causes normal points to be incorrectly identified as outliers. This typically happens when outliers inflate the variance, making the detection threshold too sensitive.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Generate clustered data with one extreme outlier
np.random.seed(42)
cluster1 = np.random.normal(0, 0.5, 50)
cluster2 = np.random.normal(5, 0.5, 50)
extreme_outlier = [20]  # This will cause swamping

data = np.concatenate([cluster1, cluster2, extreme_outlier])

# Z-score detection (affected by swamping)
z_scores = np.abs(stats.zscore(data))
outliers_zscore = data[z_scores > 2]

# Robust detection using median and MAD
median_val = np.median(data)
mad = np.median(np.abs(data - median_val))
modified_z_scores = 0.6745 * (data - median_val) / mad
outliers_mad = data[np.abs(modified_z_scores) > 2]

print(f"Z-score outliers (swamping): {len(outliers_zscore)} points")
print(f"MAD outliers (robust): {len(outliers_mad)} points")
```

```output
Z-score outliers (swamping): 1 points
MAD outliers (robust): 1 points
Z-score detected: [20.]
MAD detected: [20.]
```

The extreme outlier 20 inflated variance, causing normal points to be flagged as outliers

![Swamping Example](/images/swamping_example.png)

## Labels & Scores

A label is a binary classification of whether a record is an outlier or not.  Generating a label is often done with a score and a threshold.

A score is a continuous value that indicates how abnormal a record is.  Example of scores include a z score, a distance from a cluster, or a density estimate.

Scores can be combined and ranked, although care is needed when combining scores on different scales.  Scores can be combined with the sum, maximum or sum of squares.

## Univariate vs. Multivariate Anomaly Detection

Univariate outlier detection uses a single feature to find unusual values.  

The scores created with univariate anomaly detection can be combined for all features in a row, to generate an estimate for the entire row. A univariate outlier detection model cannot understand the relationships between features.

Multivariate outlier detection uses many features, and can find unusual combinations of unusual values alongside just finding unusual values.

Multivariate outlier detection has two sources of unusualness - unusual because of the single feature and/or unusual when combined.

## Curse of Dimensionality

The curse of dimensionality refers to the problems that arise when working with high-dimensional data.

Very few outliers require many features to detect them.  Some outliers can only be detected in high dimensions.

For outlier detection, high dimensions cause the following issues:

- **Interpretability**: Outliers that relate to many features are less interpretable
- **Sparsity**: High dimensional data can become spares, where many combinations cannot be meaningfully populated. This leads
- more dimensions -> more ways to be unusual

## Categorical Features

Categorical features deserve a special mention.

One common way to detect anomalies with categorical variables is with counts.  Anomalies can then be detected by thresholds on the counts.

Can set a threshold on a cumulative count. Here you would set a record to an anomaly based on the cumulative count of all values of that column, stopping where the total is equal to a threshold of 1% (for example).

You can also do outlier detection on the counts - for example using MAD or z score on the counts.

The final method here is the marginal probabilities (page 115)

## Visualizing Anomalies

Visualization is crucial for understanding your data and validating anomaly detection results. Different chart types work best for different data combinations:

**Single variable visualizations:**
- **Histograms**: Show the distribution and identify gaps or unusual peaks
- **Kernel Density Estimation (KDE)**: Smooth density curves that highlight low-density regions

**Variable combination visualizations:**
- **Two categorical variables**: Heatmaps show frequency patterns
- **Two numeric variables**: Scatterplots reveal outlying points and clusters  
- **One categorical, one numeric**: Boxplots identify outliers within categories

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.datasets import make_blobs

# Set random seed for reproducibility
np.random.seed(42)

# Generate sample data with outliers
normal_data = np.random.normal(50, 10, 1000)
outliers = [20, 25, 85, 90]
univariate_data = np.concatenate([normal_data, outliers])

# Generate bivariate data with outliers
centers = [[0, 0], [3, 3]]
cluster_data, _ = make_blobs(n_samples=200, centers=centers, 
                            cluster_std=0.8, random_state=42)
bivariate_outliers = np.array([[6, 1], [-2, 4], [1, -3]])
bivariate_data = np.vstack([cluster_data, bivariate_outliers])

# Generate categorical data
categories = ['A', 'B', 'C', 'D']
cat_counts = [150, 120, 100, 5]  # 'D' is an outlier category
categorical_data = []
for cat, count in zip(categories, cat_counts):
    categorical_data.extend([cat] * count)

# Create visualization plots
fig = plt.figure(figsize=(15, 12))

# 1. Histogram
plt.subplot(3, 3, 1)
plt.hist(univariate_data, bins=30, alpha=0.7, edgecolor='black')
plt.title('Histogram: Univariate Data with Outliers')
plt.xlabel('Value')
plt.ylabel('Frequency')

# 2. KDE
plt.subplot(3, 3, 2)
sns.histplot(univariate_data, kde=True, alpha=0.7)
plt.title('KDE: Density Estimation')
plt.xlabel('Value')
plt.ylabel('Density')

# 3. Boxplot
plt.subplot(3, 3, 3)
plt.boxplot(univariate_data)
plt.title('Boxplot: Outlier Detection')
plt.ylabel('Value')

# 4. Scatterplot (two numeric)
plt.subplot(3, 3, 4)
plt.scatter(bivariate_data[:-3, 0], bivariate_data[:-3, 1], 
           alpha=0.6, label='Normal points')
plt.scatter(bivariate_data[-3:, 0], bivariate_data[-3:, 1], 
           color='red', s=100, label='Outliers')
plt.title('Scatterplot: Two Numeric Variables')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

# 5. Categorical frequency bar chart
plt.subplot(3, 3, 5)
unique_cats, counts = np.unique(categorical_data, return_counts=True)
colors = ['red' if c < 50 else 'blue' for c in counts]
plt.bar(unique_cats, counts, color=colors)
plt.title('Categorical Data: Frequency by Category')
plt.xlabel('Category')
plt.ylabel('Count')

# 6. Heatmap simulation (two categorical)
plt.subplot(3, 3, 6)
# Create a correlation matrix as example
corr_matrix = np.random.rand(4, 4)
corr_matrix[3, :] = 0.1  # Make one category unusual
corr_matrix[:, 3] = 0.1
sns.heatmap(corr_matrix, annot=True, cmap='viridis', 
           xticklabels=categories, yticklabels=categories)
plt.title('Heatmap: Two Categorical Variables')

# 7. Mixed: Categorical vs Numeric (boxplot by category)
plt.subplot(3, 3, 7)
mixed_data = []
mixed_categories = []
for i, (cat, count) in enumerate(zip(categories, cat_counts)):
    if cat == 'D':  # Outlier category has different distribution
        values = np.random.normal(80, 5, count)
    else:
        values = np.random.normal(50, 10, count)
    mixed_data.extend(values)
    mixed_categories.extend([cat] * count)

# Create boxplot data
boxplot_data = [mixed_data[i:i+cat_counts[j]] 
               for j, cat in enumerate(categories) 
               for i in [sum(cat_counts[:j])]]
plt.boxplot([mixed_data[sum(cat_counts[:i]):sum(cat_counts[:i+1])] 
            for i in range(len(categories))], 
           labels=categories)
plt.title('Boxplot: Categorical vs Numeric')
plt.xlabel('Category')
plt.ylabel('Value')

# 8. Z-score visualization
plt.subplot(3, 3, 8)
z_scores = np.abs(stats.zscore(univariate_data))
plt.scatter(range(len(univariate_data)), univariate_data, 
           c=z_scores, cmap='Reds', alpha=0.6)
plt.axhline(np.mean(univariate_data) + 2*np.std(univariate_data), 
           color='red', linestyle='--', label='2σ threshold')
plt.axhline(np.mean(univariate_data) - 2*np.std(univariate_data), 
           color='red', linestyle='--')
plt.title('Z-score Visualization')
plt.xlabel('Data Point Index')
plt.ylabel('Value')
plt.colorbar(label='|Z-score|')

# 9. Density contour plot
plt.subplot(3, 3, 9)
from scipy.stats import gaussian_kde
# Create density estimation
xy = np.vstack([bivariate_data[:-3, 0], bivariate_data[:-3, 1]])
kde = gaussian_kde(xy)
xi, yi = np.mgrid[bivariate_data[:, 0].min():bivariate_data[:, 0].max():50j,
                  bivariate_data[:, 1].min():bivariate_data[:, 1].max():50j]
zi = kde(np.vstack([xi.flatten(), yi.flatten()]))
plt.contour(xi, yi, zi.reshape(xi.shape), alpha=0.6)
plt.scatter(bivariate_data[:-3, 0], bivariate_data[:-3, 1], 
           alpha=0.6, label='Normal points')
plt.scatter(bivariate_data[-3:, 0], bivariate_data[-3:, 1], 
           color='red', s=100, label='Outliers')
plt.title('Density Contours with Outliers')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.tight_layout()
plt.savefig('/Users/adamgreen/data-science-south-neu/static/images/anomaly_visualizations.png', 
           dpi=300, bbox_inches='tight')

print("Visualization examples generated:")
print(f"- Univariate data points: {len(univariate_data)}")
print(f"- Bivariate data points: {len(bivariate_data)}")
print(f"- Categorical data points: {len(categorical_data)}")
print(f"- Outliers in univariate: {outliers}")
print(f"- Outliers in bivariate: {bivariate_outliers.tolist()}")
```

Output:
```
Visualization examples generated:
- Univariate data points: 1004
- Bivariate data points: 203
- Categorical data points: 375
- Outliers in univariate: [20, 25, 85, 90]
- Outliers in bivariate: [[6, 1], [-2, 4], [1, -3]]
```

![Anomaly Visualization Examples](/images/anomaly_visualizations.png)

## Anomaly Detection Methods

Anomaly detection methods can be categorized into three main approaches, each with different strengths and use cases:

- **Rules-based**: Simple, interpretable thresholds and conditions
- **Statistical**: Mathematical approaches using distributions and probabilities  
- **Machine Learning**: Advanced algorithms that learn patterns from data

The choice of method depends on your data characteristics, domain knowledge, interpretability requirements, and computational constraints.

## Rules-Based Detection

Rules-based anomaly detection uses predefined thresholds and logical conditions to identify outliers. These methods are highly interpretable and easy to implement.

### Univariate Rules

Simple threshold-based rules applied to single variables:

- **Range rules**: Flag values outside expected ranges (e.g., age < 0 or age > 150)
- **Percentage rules**: Flag extreme percentiles (e.g., top/bottom 5%)
- **Business logic rules**: Domain-specific constraints (e.g., salary > revenue for a company)

### Multivariate Rules

Logical combinations of multiple variables:

- **Conditional rules**: IF condition THEN threshold (e.g., IF age < 18 THEN income should be < $50k)
- **Ratio rules**: Relationships between variables (e.g., debt-to-income ratio > 0.4)
- **Consistency rules**: Cross-field validation (e.g., end_date must be after start_date)

**Advantages:**
- **Interpretable**: Easy to understand and explain to stakeholders
- **Fast**: Simple comparisons with minimal computation
- **Customizable**: Can incorporate domain expertise and business logic
- **Deterministic**: Same input always produces same result

**Disadvantages:**
- **Manual effort**: Requires domain knowledge to set appropriate thresholds
- **Rigid**: May miss novel anomaly patterns
- **Complexity explosion**: Multivariate rules become difficult to manage with many features
- **Threshold sensitivity**: Performance depends heavily on chosen thresholds

## Statistical Methods

Statistical anomaly detection uses mathematical properties of data distributions to identify outliers. These methods assume data follows certain statistical patterns and flag points that deviate significantly from these patterns.

### Z-Score (Standard Score)

Measures how many standard deviations a point is from the mean. Points with |z-score| > 2 or 3 are typically considered outliers.

**Formula:** `z = (x - μ) / σ`

**Advantages:** Simple, well-understood, works well for normal distributions  
**Disadvantages:** Sensitive to extreme outliers (masking/swamping), assumes normal distribution

### Interquartile Range (IQR)

Uses quartiles to define outlier boundaries. Points outside Q1 - 1.5×IQR or Q3 + 1.5×IQR are flagged.

**Formula:** `IQR = Q3 - Q1`, outliers if `x < Q1 - 1.5×IQR` or `x > Q3 + 1.5×IQR`

**Advantages:** Robust to outliers, doesn't assume normal distribution  
**Disadvantages:** Fixed multiplier (1.5) may not suit all datasets

### Median Absolute Deviation (MAD)

More robust alternative to standard deviation, using median instead of mean.

**Formula:** `MAD = median(|x - median(x)|)`

**Advantages:** Highly robust to outliers, works with skewed distributions  
**Disadvantages:** Less intuitive than z-score, requires threshold tuning

### Modified Z-Score

Uses MAD instead of standard deviation for more robust outlier detection.

**Formula:** `Modified z = 0.6745 × (x - median(x)) / MAD`

**Advantages:** Combines interpretability of z-score with robustness of MAD  
**Disadvantages:** Constant (0.6745) assumes normal distribution

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Generate data with outliers
np.random.seed(42)
normal_data = np.random.normal(50, 10, 1000)
outliers = [10, 15, 90, 95, 100]
data = np.concatenate([normal_data, outliers])

# 1. Z-Score Method
z_scores = np.abs(stats.zscore(data))
z_outliers = data[z_scores > 2]

# 2. IQR Method
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
iqr_outliers = data[(data < lower_bound) | (data > upper_bound)]

# 3. MAD Method
median = np.median(data)
mad = np.median(np.abs(data - median))
mad_outliers = data[np.abs(data - median) > 3 * mad]

# 4. Modified Z-Score Method
modified_z_scores = 0.6745 * (data - median) / mad
modified_z_outliers = data[np.abs(modified_z_scores) > 3.5]

print("Statistical Outlier Detection Results:")
print(f"Z-Score outliers ({len(z_outliers)}): {sorted(z_outliers)[:10]}")
print(f"IQR outliers ({len(iqr_outliers)}): {sorted(iqr_outliers)[:10]}")
print(f"MAD outliers ({len(mad_outliers)}): {sorted(mad_outliers)[:10]}")
print(f"Modified Z-Score outliers ({len(modified_z_outliers)}): {sorted(modified_z_outliers)[:10]}")

# Statistical summary
print(f"\nData statistics:")
print(f"Mean: {np.mean(data):.2f}, Std: {np.std(data):.2f}")
print(f"Median: {median:.2f}, MAD: {mad:.2f}")
print(f"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}")
```

### Combining Statistical Methods

Statistical scores can be combined across multiple features to create multivariate outlier detection:

- **Sum of scores**: Add z-scores across features
- **Maximum score**: Take the highest absolute score across features  
- **Sum of squares**: Combine squared z-scores (similar to Mahalanobis distance)

**Advantages of Statistical Methods:**
- **Well-understood**: Based on established statistical theory
- **Computationally efficient**: Simple calculations, fast execution
- **Threshold interpretability**: Scores have statistical meaning
- **Robust options available**: MAD and IQR handle outliers well

**Disadvantages:**
- **Distribution assumptions**: Many methods assume normal distributions
- **Univariate focus**: Don't capture relationships between variables
- **Parameter sensitivity**: Threshold choice affects performance significantly

## Machine Learning Methods

Machine learning approaches to anomaly detection learn patterns from data and can handle complex, multivariate relationships. These algorithms are typically more sophisticated but less interpretable than rules or statistical methods.

### Distance-Based Methods

Identify outliers based on their distance to other points or clusters.

**k-Nearest Neighbors (k-NN):**
- Flag points with large distances to their k nearest neighbors
- Can use mean, median, or maximum distance to k neighbors
- Works well when normal points form dense clusters

**Advantages:** Simple concept, handles arbitrary cluster shapes  
**Disadvantages:** Computationally expensive, sensitive to k parameter, struggles with varying density

### Density-Based Methods

Identify outliers as points in low-density regions.

**Local Outlier Factor (LOF):**
- Compares local density of a point to densities of its neighbors
- Points in sparser regions relative to neighbors are flagged
- Handles clusters of different densities well

**Kernel Density Estimation (KDE):**
- Estimates probability density function and flags low-probability points
- Can capture complex data distributions

**Advantages:** Handles varying cluster densities, finds local anomalies  
**Disadvantages:** Parameter tuning required, computationally intensive

### Clustering-Based Methods

Use clustering algorithms to identify outliers as points that don't belong to any cluster or are far from cluster centers.

**k-Means Outliers:**
- Points far from nearest cluster centroid
- Points in small or sparse clusters

**DBSCAN Outliers:**
- Points classified as "noise" by the algorithm
- Points not belonging to any dense cluster

**Advantages:** Leverages existing clustering algorithms, intuitive concept  
**Disadvantages:** Performance depends on clustering quality, parameter sensitive

### Tree-Based Methods

**Isolation Forest:**
- Isolates anomalies by randomly partitioning data
- Anomalies require fewer partitions to isolate (shorter paths in trees)
- Ensemble of isolation trees votes on anomaly score

**Advantages:** Fast, handles large datasets, no assumptions about data distribution  
**Disadvantages:** Less interpretable, struggles with normal points in sparse regions

### Probabilistic Methods

**Gaussian Mixture Models (GMM):**
- Model data as mixture of Gaussian distributions
- Flag points with low likelihood under the learned model
- Can capture multiple modes in data

**One-Class SVM:**
- Learn boundary around normal data in high-dimensional space
- Flag points outside the learned boundary

**Advantages:** Principled probabilistic framework, can handle complex distributions  
**Disadvantages:** Strong distributional assumptions, computationally intensive

### Histogram-Based Methods

**Histogram-Based Outlier Score (HBOS):**
- Build histograms for each feature independently
- Combine scores across features (assumes feature independence)
- Fast and interpretable for high-dimensional data

**Advantages:** Fast, scales well, interpretable feature contributions  
**Disadvantages:** Assumes feature independence, may miss multivariate patterns

```python
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.cluster import DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# Generate sample data with outliers
np.random.seed(42)
X_normal, _ = make_blobs(n_samples=300, centers=2, cluster_std=1.0, random_state=42)
X_outliers = np.random.uniform(low=-6, high=6, size=(20, 2))
X = np.vstack([X_normal, X_outliers])

# 1. Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
iso_predictions = iso_forest.fit_predict(X)
iso_outliers = X[iso_predictions == -1]

# 2. Local Outlier Factor
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
lof_predictions = lof.fit_predict(X)
lof_outliers = X[lof_predictions == -1]

# 3. DBSCAN (noise points as outliers)
dbscan = DBSCAN(eps=0.8, min_samples=5)
dbscan_labels = dbscan.fit_predict(X)
dbscan_outliers = X[dbscan_labels == -1]

# 4. Gaussian Mixture Model
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(X_normal)  # Train only on normal data
log_likelihood = gmm.score_samples(X)
threshold = np.percentile(log_likelihood, 10)  # Bottom 10% as outliers
gmm_outliers = X[log_likelihood < threshold]

print("Machine Learning Outlier Detection Results:")
print(f"Isolation Forest outliers: {len(iso_outliers)}")
print(f"Local Outlier Factor outliers: {len(lof_outliers)}")
print(f"DBSCAN noise points: {len(dbscan_outliers)}")
print(f"GMM low-likelihood outliers: {len(gmm_outliers)}")

# Performance comparison
true_outliers = len(X_outliers)  # We know we added 20 outliers
print(f"\nActual outliers in data: {true_outliers}")
print(f"Detection accuracy comparison:")
print(f"- Isolation Forest: {len(iso_outliers)}/{true_outliers}")
print(f"- LOF: {len(lof_outliers)}/{true_outliers}")
print(f"- DBSCAN: {len(dbscan_outliers)}/{true_outliers}")
print(f"- GMM: {len(gmm_outliers)}/{true_outliers}")
```

**Advantages of Machine Learning Methods:**
- **Multivariate**: Capture complex relationships between features
- **Adaptive**: Learn patterns from data without manual threshold setting
- **Flexible**: Handle non-linear relationships and arbitrary distributions
- **Scalable**: Many algorithms work well with large datasets

**Disadvantages:**
- **Less interpretable**: Harder to explain why a point is flagged
- **Parameter tuning**: Requires hyperparameter optimization
- **Computational cost**: More expensive than statistical methods
- **Overfitting risk**: May learn noise patterns in training data
- **Data requirements**: Need sufficient training data for reliable patterns

## Practical Tips

Can usually be applied when you have lots of data

Don't expect no anomalies - threshold on normal expected anomalies

Where possible, separate out records and do outlire detection separately
- purchase and sale records - should outlire detection two times, not 1

### Ensembles

Ensembles help
- can see different patterns
- average through variance in model results
- more reliable final score

## Explainability & Intepretability

Chapter 13

What makes a model explainable or interpretable depends on the audience.

Global vs Local Explanations
- Evaluator as a whole, versus an individual record 

### Interpretability

Model is inherently explainable

Univariate tests are interpretable (QR, MAD, HBOS, ECOD, FPOF)

Preferred

bhad - bayesian histogram anomaly detection
- is interpretable

countsoutlierdetector
- histogram based detector

dataconsistencychecker

### Explainability

Add techniques to explain why a detector does what it does

feature importances
- dont look at interactions between features

SHAP

proxy model - interpetable predictive model that predicts predictions made by another model
- can do with rules, decision trees

visualizations 
- Alibi (https://github.com/SeldonIO/alibi)
- partial dependence plots
- inidvidual conditional expectation plots
- accumulated local effects plots

counterfactuals
- Alibi (https://github.com/SeldonIO/alibi)
- minimum changes to a row needed to produce a different prediction
